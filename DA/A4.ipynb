{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "d74a95a9-9059-4b23-b336-1e8cea557842",
      "cell_type": "code",
      "source": "# 1. Consider any text paragraph. Preprocess the text to remove any special characters and digits. Generate the summary using extractive summarization process.\n\nimport nltk\nimport re\nimport heapq\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n# Ensure necessary NLTK packages are downloaded\n# nltk.download('punkt')\n# nltk.download('stopwords')\n\n# Our text paragraph\nparagraph = \"\"\"\nThe field of artificial intelligence (AI) is rapidly transforming the world around us. From self-driving cars to personalized medicine, AI's potential seems limitless. However, this rapid advancement also brings significant challenges. One of the primary concerns is the impact on the workforce, as automation could displace millions of jobs. Furthermore, ethical considerations surrounding bias in algorithms and data privacy are becoming increasingly important. It is crucial for researchers, policymakers, and the public to engage in a continuous dialogue to ensure that AI develops in a way that benefits all of humanity. The decisions we make today will shape the future of this powerful technology.\n\"\"\"\n\n# 1. Preprocess the text: remove special characters and digits\n# Remove square brackets and digits (like [0-9]*)\nformatted_text = re.sub(r'\\[[0-9]*\\]', ' ', paragraph)\n# Remove special characters and digits, keep only letters and spaces\nformatted_text = re.sub(r'[^a-zA-Z\\s]', ' ', formatted_text)\n# Remove extra whitespace\nformatted_text = re.sub(r'\\s+', ' ', formatted_text)\n\nprint(\"--- Preprocessed Text ---\")\nprint(formatted_text)\nprint(\"-\" * 30)\n\n# 2. Tokenize the preprocessed text into sentences and words\nsentences = sent_tokenize(paragraph) # Use original for sentence structure, but cleaned for words\nwords = word_tokenize(formatted_text.lower()) # Use lowercased, cleaned text for frequency\n\n# 3. Create a word frequency table, removing stopwords\nstop_words = set(stopwords.words(\"english\"))\nword_frequencies = {}\n\nfor word in words:\n    if word not in stop_words:\n        if word not in word_frequencies:\n            word_frequencies[word] = 1\n        else:\n            word_frequencies[word] += 1\n\n# 4. Compute weighted frequencies (normalize by the most frequent word)\nif word_frequencies:\n    maximum_frequency = max(word_frequencies.values())\n    for word in word_frequencies.keys():\n        word_frequencies[word] = word_frequencies[word] / maximum_frequency\n\n# 5. Score sentences based on the weighted frequencies of words they contain\nsentence_scores = {}\nfor sent in sentences:\n    for word, freq in word_frequencies.items():\n        if word in sent.lower():\n            if sent not in sentence_scores:\n                sentence_scores[sent] = freq\n            else:\n                sentence_scores[sent] += freq\n\n# 6. Generate the summary by selecting the top N sentences (e.g., top 2)\nif sentence_scores:\n    summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)\n    summary = ' '.join(summary_sentences)\n    print(\"\\n--- Generated Summary ---\")\n    print(summary)\nelse:\n    print(\"\\nCould not generate summary.\")\n    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "--- Preprocessed Text ---\n The field of artificial intelligence AI is rapidly transforming the world around us From self driving cars to personalized medicine AI s potential seems limitless However this rapid advancement also brings significant challenges One of the primary concerns is the impact on the workforce as automation could displace millions of jobs Furthermore ethical considerations surrounding bias in algorithms and data privacy are becoming increasingly important It is crucial for researchers policymakers and the public to engage in a continuous dialogue to ensure that AI develops in a way that benefits all of humanity The decisions we make today will shape the future of this powerful technology \n------------------------------\n"
        },
        {
          "ename": "<class 'LookupError'>",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/pyodide/nltk_data'\n    - '/nltk_data'\n    - '/share/nltk_data'\n    - '/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 2. Tokenize the preprocessed text into sentences and words\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m sentences = \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use original for sentence structure, but cleaned for words\u001b[39;00m\n\u001b[32m     32\u001b[39m words = word_tokenize(formatted_text.lower()) \u001b[38;5;66;03m# Use lowercased, cleaned text for frequency\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 3. Create a word frequency table, removing stopwords\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/pyodide/nltk_data'\n    - '/nltk_data'\n    - '/share/nltk_data'\n    - '/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "efe0549b-1c2e-458a-9439-40a9a5aad65a",
      "cell_type": "code",
      "source": "# 2. Consider any text paragraph. Remove the stopwords. Tokenize the paragraph to extract words and sentences. Calculate the word frequency distribution and plot the frequencies. Plot the wordcloud of the text.\n\nimport nltk\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud\n\n# Ensure necessary NLTK packages are downloaded\n# nltk.download('punkt')\n# nltk.download('stopwords')\n\n# Our text paragraph\nparagraph = \"\"\"\nThe rainforest is a lush, green world teeming with life. Tall trees stretch towards the sky, their branches forming a dense canopy. Beneath this, a world of shadows and moisture thrives, filled with exotic plants, insects, and animals. The sounds of the forest are a constant symphony: the calls of monkeys, the buzz of insects, and the songs of colorful birds. This vibrant ecosystem is vital for the planet's health, producing oxygen and regulating the climate.\n\"\"\"\n\nprint(\"--- Original Paragraph ---\")\nprint(paragraph)\nprint(\"-\" * 50)\n\n# 1. Tokenize the paragraph into sentences and words\ntokenized_sentences = sent_tokenize(paragraph)\ntokenized_words = word_tokenize(paragraph)\n\nprint(\"--- Tokenized Sentences ---\")\nprint(tokenized_sentences)\nprint(\"\\n--- Tokenized Words (First 20) ---\")\nprint(tokenized_words[:20])\n\n# 2. Remove stopwords\nstop_words = set(stopwords.words(\"english\"))\nfiltered_words = [word for word in tokenized_words if word.lower() not in stop_words and word.isalpha()] # Keep only alphabetic words\n\nprint(\"\\n--- Words after Stopword Removal (First 20) ---\")\nprint(filtered_words[:20])\n\n# 3. Calculate and plot the word frequency distribution\nif filtered_words:\n    frequency_distribution = FreqDist(filtered_words)\n    print(\"\\n--- Most Common Words ---\")\n    print(frequency_distribution.most_common(5))\n\n    print(\"\\n--- Plotting Frequency Distribution ---\")\n    frequency_distribution.plot(15, cumulative=False) # Plot top 15\n    plt.show()\n\n    # 4. Plot the WordCloud\n    print(\"--- Generating WordCloud ---\")\n    # Join the filtered words back into a single string for the wordcloud\n    text_for_cloud = ' '.join(filtered_words)\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_for_cloud)\n\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\nelse:\n    print(\"No words left after filtering to generate frequency plot or wordcloud.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6052029c-7b11-44af-818b-4fcdf997d9fa",
      "cell_type": "code",
      "source": "# 3. Consider the following review messages. Perform sentiment analysis on the messages.\n# i. I purchased headphones online. I am very happy with the product.\n# ii. I saw the movie yesterday. The animation was really good but the script was ok.\n# iii. I enjoy listening to music\n# iv. I take a walk in the park everyday\n\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Ensure the VADER lexicon is downloaded\n# nltk.download('vader_lexicon')\n\n# Initialize the VADER sentiment analyzer\nanalyzer = SentimentIntensityAnalyzer()\n\n# List of review messages\nreviews = [\n    \"I purchased headphones online. I am very happy with the product.\",\n    \"I saw the movie yesterday. The animation was really good but the script was ok.\",\n    \"I enjoy listening to music\",\n    \"I take a walk in the park everyday\"\n]\n\nprint(\"--- Sentiment Analysis Results ---\")\nfor i, review in enumerate(reviews):\n    print(f\"\\nReview {i+1}: \\\"{review}\\\"\")\n    \n    # Get sentiment scores\n    sentiment_scores = analyzer.polarity_scores(review)\n    \n    # Determine overall sentiment based on compound score\n    if sentiment_scores['compound'] >= 0.05:\n        overall = \"Positive\"\n    elif sentiment_scores['compound'] <= -0.05:\n        overall = \"Negative\"\n    else:\n        overall = \"Neutral\"\n    \n    # Print the scores and overall sentiment\n    print(f\"  Scores: {sentiment_scores}\")\n    print(f\"  Overall Sentiment: {overall}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1c96754b-77fc-4190-9521-746436f464cb",
      "cell_type": "code",
      "source": "# 4. Perform text analytics on WhatsApp data : Write a Python script for the following :\n# i. First Export the WhatsApp chat of any group. Read the exported \".txt\" file using open() and read() functions.\n# ii. Tokenize the read data into sentences and print it.\n# iii. Remove the stopwords from data and perform lemmatization.\n# iv. Plot the wordcloud for the given data.\n\nimport nltk\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\n\n# Ensure necessary NLTK packages are downloaded\n# nltk.download('punkt')\n# nltk.download('stopwords')\n# nltk.download('wordnet')\n\n# --- i. Read the WhatsApp chat file ---\n# IMPORTANT: Replace 'whatsapp_chat.txt' with the actual path to your exported chat file.\n# WhatsApp chats often have datetime stamps, so we'll do basic cleaning to focus on the messages.\nfile_path = 'whatsapp_chat.txt'  # <-- UPDATE THIS PATH\n\ntry:\n    with open(file_path, 'r', encoding='utf-8') as file:\n        chat_data = file.read()\n    print(\"--- Chat File Read Successfully ---\")\n    # print(chat_data[:500]) # Print first 500 chars to see the format\n\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {file_path}. Please update the file path.\")\n    exit()\n\n# Basic cleaning: Remove common WhatsApp timestamp patterns (e.g., [DD/MM/YY, HH:MM:SS] or DD/MM/YY, HH:MM - )\n# This is a simplified example; actual timestamps may vary.\ncleaned_text = re.sub(r'\\[\\d{1,2}\\/\\d{1,2}\\/\\d{2,4},?\\s*\\d{1,2}:\\d{2}(?::\\d{2})?[^\\]]*\\]\\s*', '', chat_data)\ncleaned_text = re.sub(r'\\d{1,2}\\/\\d{1,2}\\/\\d{2,4},?\\s*\\d{1,2}:\\d{2}\\s*-\\s*', '', cleaned_text)\n# Remove \"Sender: \" patterns (optional)\ncleaned_text = re.sub(r'^[^:]+:\\s*', '', cleaned_text, flags=re.MULTILINE)\n\n\n# --- ii. Tokenize the data into sentences ---\nsentences = sent_tokenize(cleaned_text)\nprint(f\"\\n--- Tokenized into {len(sentences)} Sentences (First 5) ---\")\nfor i, sent in enumerate(sentences[:5]):\n    print(f\"{i+1}. {sent}\")\n\n# --- iii. Remove stopwords and perform lemmatization ---\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\n# Tokenize all words from the cleaned text for further processing\nall_words = word_tokenize(cleaned_text.lower()) # Lowercase for consistency\n\nprocessed_words = []\nfor word in all_words:\n    # Keep only alphabetic words and remove stopwords\n    if word.isalpha() and word not in stop_words:\n        # Perform lemmatization (treating as verb 'v' as a common case, but can be refined)\n        lemmatized_word = lemmatizer.lemmatize(word, pos='v')\n        processed_words.append(lemmatized_word)\n\nprint(f\"\\n--- Processed {len(processed_words)} words after stopword removal and lemmatization (First 20) ---\")\nprint(processed_words[:20])\n\n# --- iv. Plot the wordcloud for the processed data ---\nif processed_words:\n    print(\"\\n--- Generating WordCloud ---\")\n    text_for_cloud = ' '.join(processed_words)\n    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text_for_cloud)\n\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(\"WordCloud of WhatsApp Chat\")\n    plt.show()\nelse:\n    print(\"No words left after processing to generate wordcloud.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}